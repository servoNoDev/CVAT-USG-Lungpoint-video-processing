{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random as rand\n",
    "import ast\n",
    "from typing import Any, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVAT_FOLDER_PATH = \"/home/mh731nk/_data/experiments_tmp/data/revision_8/cvat_project_raw_unzip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read video DataFrame \n",
    "Data was prepared in the previous part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344, 12)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset path\n",
    "df_videos = pd.read_pickle(\"/home/mh731nk/_data/experiments_tmp/data/revision_8/video.pkl\", 'zip')\n",
    "df_videos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select videos for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP -> 17\n",
      "A -> 9\n",
      "P -> 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtere_videos = df_videos.loc[df_videos[\"folder_class\"].isin([\n",
    "    'USG - Lung point',\n",
    "    'USG - Lung sliding absent (aj ine znaky - radiol. klin.)',\n",
    "    'USG - Lung sliding present (aj ine znaky - radiol. klin.)'\n",
    "    ])]\n",
    "\n",
    "print(f'LP -> {df_filtere_videos.loc[df_filtere_videos[\"folder_class\"] == \"USG - Lung point\"].shape[0]}')\n",
    "print(f'A -> {df_filtere_videos.loc[df_filtere_videos[\"folder_class\"] == \"USG - Lung sliding absent (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "print(f'P -> {df_filtere_videos.loc[df_filtere_videos[\"folder_class\"] == \"USG - Lung sliding present (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "17 + 9 + 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read anotations for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_generator(self, size: int = 8, chars: str = string.ascii_uppercase + string.digits) -> str:\n",
    "    \"\"\"\n",
    "    Generate a random string identifier.\n",
    "\n",
    "    This method generates a random string of the specified size using a secure random\n",
    "    generator. By default, the generated string consists of uppercase letters and digits.\n",
    "\n",
    "    Args:\n",
    "        size (int): The length of the generated string. Defaults to 8.\n",
    "        chars (str): A string containing the set of characters to choose from.\n",
    "                     Defaults to uppercase letters and digits (A-Z, 0-9).\n",
    "\n",
    "    Returns:\n",
    "        str: A randomly generated string identifier.\n",
    "    \"\"\"\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "def parse_polygon_points(pts: Union[str, List[Any]]) -> List[Tuple[Union[int, float], Union[int, float]]]:\n",
    "    \"\"\"\n",
    "    Parse and normalize polygon points into a list of (x, y) coordinate pairs.\n",
    "    \n",
    "    The input can be provided in different formats:\n",
    "    - A string representation of a list (e.g., \"[556.9, 200.3, 555.3, 214.0, ...]\"), \n",
    "      which will be evaluated into a Python list.\n",
    "    - A flat list of numbers representing coordinates (e.g., [x0, y0, x1, y1, ...]).\n",
    "    - A list already containing (x, y) pairs.\n",
    "    \n",
    "    The function ensures that the returned value is always a list of tuples,\n",
    "    where each tuple represents a point as (x, y).\n",
    "    \n",
    "    Args:\n",
    "        pts (Union[str, List[Any]]): The polygon points in one of the accepted formats.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[Union[int, float], Union[int, float]]]: A list of (x, y) coordinate pairs.\n",
    "    \"\"\"\n",
    "    # If pts is a string, safely evaluate it to convert the string into a Python list.\n",
    "    if isinstance(pts, str):\n",
    "        pts = ast.literal_eval(pts)\n",
    "    \n",
    "    # If the list is a flat list of coordinates (e.g., [x0, y0, x1, y1, ...]),\n",
    "    # then group the numbers into (x, y) pairs.\n",
    "    if pts and isinstance(pts[0], (float, int)):\n",
    "        pts = list(zip(pts[::2], pts[1::2]))\n",
    "    \n",
    "    # Return the normalized list of (x, y) coordinate pairs.\n",
    "    return pts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each video exported from the CVAT annotation tool is organized into a structured folder that holds both the raw video data and its associated metadata. This structure typically includes:\n",
    "\n",
    "Raw Video Storage:\n",
    "The primary video file is saved in a dedicated folder, ensuring that the original footage is preserved intact.\n",
    "\n",
    "Metadata Files:\n",
    "Accompanying the raw video are metadata files (often in JSON format) that provide detailed information about the video, such as resolution, frame rate, and other relevant properties.\n",
    "\n",
    "Annotation Data:\n",
    "In addition to video metadata, the export includes detailed annotation files. These files contain information about label masks and polygon coordinates for each frame, which outline the annotated regions of interest within the video. This structured annotation data is essential for tasks like object detection and segmentation.\n",
    "\n",
    "This systematic folder organization makes it easy to access and process both the video content and its annotations, streamlining workflows for analysis and machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing of 52 videos\n",
      "Dataset include 3198 polygons.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold rows extracted from annotations\n",
    "rows_to_append = []\n",
    "print(f\"Start processing of {df_filtere_videos.shape[0]} videos\")\n",
    "# Iterate over each row in the DataFrame containing filtered videos\n",
    "indexer = 0\n",
    "for index, video_df_row in df_filtere_videos.iterrows():\n",
    "    indexer = indexer +1\n",
    "    # Build the path to the 'data' folder within the current video's subfolder\n",
    "    data_folder_path = os.path.join(\n",
    "        CVAT_FOLDER_PATH, \n",
    "        video_df_row[\"video_subfolder_path\"], \n",
    "        'data'\n",
    "    )\n",
    "    \n",
    "    # Walk through the directory tree starting at data_folder_path.\n",
    "    # os.walk returns tuples of (current_path, directories, files)\n",
    "    folders_scan = [x for x in os.walk(data_folder_path)]\n",
    "    \n",
    "    # Retrieve the first subdirectory from the scan (assumes at least one exists)\n",
    "    video_folder = folders_scan[0][1][0]\n",
    "    \n",
    "    # Print the name of the video folder (for debugging or logging purposes)\n",
    "    # print(f' Video {video_df_row[\"video_id\"]} is processing')\n",
    "    \n",
    "    # Construct the full path to the video subfolder\n",
    "    path_video = os.path.join(\n",
    "        CVAT_FOLDER_PATH,\n",
    "        video_df_row[\"video_subfolder_path\"]\n",
    "    )\n",
    "    \n",
    "    # Open and load the annotations JSON file\n",
    "    with open(os.path.join(path_video, 'annotations.json')) as json_file:\n",
    "        json_anotation = json.load(json_file)\n",
    "    \n",
    "    # Open and load the task JSON file\n",
    "    with open(os.path.join(path_video, 'task.json')) as json_file:\n",
    "        task = json.load(json_file)\n",
    "\n",
    "    data = json_anotation[0]  # assuming you're working with the first element\n",
    "\n",
    "    # Case 1: Shapes are directly available\n",
    "    if 'shapes' in data and data['shapes']:\n",
    "        for shape in data['shapes']:\n",
    "            row = {\n",
    "                'polygon_id': id_generator(8),             # Generate a unique identifier for the shape\n",
    "                'video_id': video_df_row[\"video_id\"],        # Retrieve the video ID from the DataFrame row\n",
    "                'name_cvat': task[\"name\"],                   # Retrieve the task name from the task JSON\n",
    "                'type': shape[\"type\"],                       # The type of shape (e.g., polygon, rectangle)\n",
    "                'frame': int(shape[\"frame\"]),                # Convert the frame number to an integer\n",
    "                'polygon_label': shape.get(\"label\", \"\"),     # Use the shape's label if available\n",
    "                'points': parse_polygon_points(list(shape[\"points\"]))  # Parse the coordinate points\n",
    "            }\n",
    "            rows_to_append.append(row)\n",
    "\n",
    "    # Case 2: Shapes are nested inside tracks\n",
    "    elif 'tracks' in data and data['tracks']:\n",
    "        for track in data['tracks']:\n",
    "            # Use the track label if individual shapes do not have one\n",
    "            track_label = track.get(\"label\", \"\")\n",
    "            for shape in track.get(\"shapes\", []):\n",
    "                row = {\n",
    "                    'polygon_id': id_generator(8),\n",
    "                    'video_id': video_df_row[\"video_id\"],\n",
    "                    'name_cvat': task[\"name\"],\n",
    "                    'type': shape[\"type\"],\n",
    "                    'frame': int(shape[\"frame\"]),\n",
    "                    # Use the shape's label if present, otherwise use the track's label\n",
    "                    'polygon_label': shape.get(\"label\", track_label),\n",
    "                    'points': parse_polygon_points(list(shape[\"points\"]))\n",
    "                }\n",
    "                rows_to_append.append(row)\n",
    "\n",
    "df_polygons = pd.DataFrame(rows_to_append) \n",
    "print(f'Dataset include {df_polygons.shape[0]} polygons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP -> 17\n",
      "A -> 9\n",
      "P -> 26\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "dffff = df_videos.loc[df_videos[\"video_id\"].isin(set(df_polygons[\"video_id\"]))]\n",
    "\n",
    "print(f'LP -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung point\"].shape[0]}')\n",
    "print(f'A -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung sliding absent (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "print(f'P -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung sliding present (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "\n",
    "print(17 + 9 + 26)\n",
    "\n",
    "# dffff.loc[df_check[\"folder_class\"] == \"USG - Lung point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>name_cvat</th>\n",
       "      <th>type</th>\n",
       "      <th>frame</th>\n",
       "      <th>polygon_label</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8P1EX91L</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>0</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.9000000000015, 200.3000000000011), (555....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YBWFZJO6</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>0</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(359.705078125, 186.00537109375), (359.704868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KU6TDXXI</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>10</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(366.041015625, 185.21337890625), (366.040805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HLXZ86YV</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>10</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.9000000000015, 201.8000000000011), (555....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LWD2X9SY</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>20</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.1000000000004, 207.40000000000146), (555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>8AJTS5PW</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>260</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(365.4625976562511, 203.77187500000036), (361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>ZLJX2TTQ</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>270</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1701171875011, 203.13808593750036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>0N49D2QN</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>280</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1701171875011, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>7RL6DA8A</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>290</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1706054687511, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>0T773Q52</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>299</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1706054687511, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3198 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polygon_id  video_id                               name_cvat     type  \\\n",
       "0      8P1EX91L  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "1      YBWFZJO6  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "2      KU6TDXXI  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "3      HLXZ86YV  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "4      LWD2X9SY  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "...         ...       ...                                     ...      ...   \n",
       "3193   8AJTS5PW  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "3194   ZLJX2TTQ  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "3195   0N49D2QN  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "3196   7RL6DA8A  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "3197   0T773Q52  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "\n",
       "      frame       polygon_label  \\\n",
       "0         0   lungslidingabsent   \n",
       "1         0   lungslidingabsent   \n",
       "2        10   lungslidingabsent   \n",
       "3        10   lungslidingabsent   \n",
       "4        20   lungslidingabsent   \n",
       "...     ...                 ...   \n",
       "3193    260  lungslidingpresent   \n",
       "3194    270  lungslidingpresent   \n",
       "3195    280  lungslidingpresent   \n",
       "3196    290  lungslidingpresent   \n",
       "3197    299  lungslidingpresent   \n",
       "\n",
       "                                                 points  \n",
       "0     [(556.9000000000015, 200.3000000000011), (555....  \n",
       "1     [(359.705078125, 186.00537109375), (359.704868...  \n",
       "2     [(366.041015625, 185.21337890625), (366.040805...  \n",
       "3     [(556.9000000000015, 201.8000000000011), (555....  \n",
       "4     [(556.1000000000004, 207.40000000000146), (555...  \n",
       "...                                                 ...  \n",
       "3193  [(365.4625976562511, 203.77187500000036), (361...  \n",
       "3194  [(371.1701171875011, 203.13808593750036), (366...  \n",
       "3195  [(371.1701171875011, 202.50332031250036), (366...  \n",
       "3196  [(371.1706054687511, 202.50332031250036), (366...  \n",
       "3197  [(371.1706054687511, 202.50332031250036), (366...  \n",
       "\n",
       "[3198 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance ID for each polygon annotation.\n",
    "\n",
    "In a single frame, multiple polygons can be annotated, potentially with different labels. Additionally, it is possible\n",
    "to have multiple polygons of the same type within the same frame. In the original dataset, polygons corresponding to the\n",
    "same observed object across different frames do not share a common reference. Assigning a unique instance ID to each\n",
    "polygon is crucial for later interpolation operations, as it allows for linking and tracking the same object across frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import List, Tuple, Union, Any\n",
    "\n",
    "def compute_polygon_centroid(poly: List[Tuple[Union[int, float], Union[int, float]]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the centroid of a polygon represented by a list of (x, y) tuples.\n",
    "    \n",
    "    The centroid is calculated as the arithmetic mean of the vertices.\n",
    "    \n",
    "    Parameters:\n",
    "        poly (List[Tuple[Union[int, float], Union[int, float]]]):\n",
    "            A list of (x, y) tuples representing the polygon's vertices.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing the (x, y) coordinates of the centroid.\n",
    "    \"\"\"\n",
    "    # Convert the list of points into a numpy array for efficient computation.\n",
    "    pts = np.array(poly)\n",
    "    # Calculate the mean of x and y coordinates separately and return as a numpy array.\n",
    "    return np.array([pts[:, 0].mean(), pts[:, 1].mean()])\n",
    "\n",
    "\n",
    "def get_polygon_from_row(row: Any) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve polygon data from a row of a DataFrame.\n",
    "    \n",
    "    This function checks for polygon data in two possible columns:\n",
    "      - 'interp_polygon': preferred if available.\n",
    "      - 'points': used if 'interp_polygon' is not available.\n",
    "    \n",
    "    Parameters:\n",
    "        row (Any): A dictionary-like object (e.g., a pandas Series) representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        List: A list representing the polygon points (assumed to be (x, y) tuples).\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If neither 'interp_polygon' nor 'points' is found in the row.\n",
    "    \"\"\"\n",
    "    # Check for interpolated polygon data first.\n",
    "    poly = row.get('interp_polygon', None)\n",
    "    if poly is not None:\n",
    "        return poly\n",
    "    # Fallback to original points if interpolated data is not available.\n",
    "    poly = row.get('points', None)\n",
    "    if poly is not None:\n",
    "        return poly\n",
    "    # Raise an error if no polygon data is found.\n",
    "    raise ValueError(\"Row does not contain polygon data.\")\n",
    "\n",
    "\n",
    "def assign_tracks_poly(df_group: pd.DataFrame, max_distance: float = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign consistent track IDs to polygon annotations across frames.\n",
    "    \n",
    "    For a given group (annotations from the same video and with the same polygon label),\n",
    "    this function assigns a unique \"polygon_track_id\" to each polygon such that the same\n",
    "    object is tracked across consecutive frames. This is achieved by comparing the centroids\n",
    "    of polygons between frames using the Hungarian algorithm to minimize the assignment cost.\n",
    "    \n",
    "    Parameters:\n",
    "        df_group (pd.DataFrame):\n",
    "            DataFrame containing polygon annotations for a single video and a specific polygon label.\n",
    "            Expected columns include \"frame\" and either \"interp_polygon\" or \"points\" for polygon data.\n",
    "        max_distance (float):\n",
    "            The maximum Euclidean distance allowed for linking two polygons across frames.\n",
    "            Polygons with a centroid distance exceeding this value are not considered the same object.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            A new DataFrame with an added \"polygon_track_id\" column that provides the track IDs.\n",
    "    \"\"\"\n",
    "    # Sort the group by frame to ensure temporal order.\n",
    "    df_group = df_group.sort_values(\"frame\").reset_index(drop=True)\n",
    "    \n",
    "    # Initialize the 'polygon_track_id' column with a default value of -1 (unassigned).\n",
    "    df_group[\"polygon_track_id\"] = -1\n",
    "    next_track_id = 0  # Counter for assigning new track IDs.\n",
    "\n",
    "    # List to store information about polygons from the previous frame.\n",
    "    # Each element is a dictionary with keys: \"track_id\" and \"centroid\".\n",
    "    prev_tracks = []\n",
    "\n",
    "    # Array to store the new track IDs corresponding to each row.\n",
    "    new_ids = np.empty(len(df_group), dtype=int)\n",
    "    \n",
    "    # Process the DataFrame frame by frame.\n",
    "    for frame, frame_df in df_group.groupby(\"frame\"):\n",
    "        # Get the indices of rows in the current frame.\n",
    "        idx = frame_df.index.tolist()\n",
    "        # List to store the computed centroids for polygons in the current frame.\n",
    "        centroids = []\n",
    "        for i in idx:\n",
    "            # Retrieve polygon data from the current row.\n",
    "            poly = get_polygon_from_row(df_group.loc[i])\n",
    "            # Compute the centroid for the polygon.\n",
    "            centroid = compute_polygon_centroid(poly)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        # Initialize an array for storing track IDs for polygons in the current frame.\n",
    "        frame_ids = np.full(len(idx), -1, dtype=int)\n",
    "        \n",
    "        if not prev_tracks:\n",
    "            # For the first frame, assign a new track ID to every polygon.\n",
    "            for j in range(len(idx)):\n",
    "                frame_ids[j] = next_track_id\n",
    "                next_track_id += 1\n",
    "        else:\n",
    "            # Build a cost matrix where each entry is the Euclidean distance between\n",
    "            # a polygon in the previous frame and one in the current frame.\n",
    "            cost_matrix = np.zeros((len(prev_tracks), len(idx)))\n",
    "            for i, prev in enumerate(prev_tracks):\n",
    "                for j, current_centroid in enumerate(centroids):\n",
    "                    cost_matrix[i, j] = np.linalg.norm(prev[\"centroid\"] - current_centroid)\n",
    "            \n",
    "            # Use the Hungarian algorithm to determine the best assignment between previous and current polygons.\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            # For each assignment, if the distance is within the allowed max_distance,\n",
    "            # assign the same track ID from the previous frame.\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if cost_matrix[r, c] <= max_distance:\n",
    "                    frame_ids[c] = prev_tracks[r][\"track_id\"]\n",
    "            \n",
    "            # For any polygon that did not receive a track ID, assign a new one.\n",
    "            for j in range(len(idx)):\n",
    "                if frame_ids[j] == -1:\n",
    "                    frame_ids[j] = next_track_id\n",
    "                    next_track_id += 1\n",
    "        \n",
    "        # Record the assigned track IDs for the current frame in the new_ids array.\n",
    "        for k, i in enumerate(idx):\n",
    "            new_ids[i] = frame_ids[k]\n",
    "        \n",
    "        # Update the prev_tracks list with polygons from the current frame for use in the next iteration.\n",
    "        prev_tracks = []\n",
    "        for j, i in enumerate(idx):\n",
    "            prev_tracks.append({\n",
    "                \"track_id\": frame_ids[j],\n",
    "                \"centroid\": centroids[j]\n",
    "            })\n",
    "    \n",
    "    # Assign the computed track IDs back to the DataFrame.\n",
    "    df_group[\"polygon_track_id\"] = new_ids\n",
    "    return df_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Example Usage -----\n",
    "# Assume df_polygons_revision_9 is a DataFrame containing polygon annotations with the following columns:\n",
    "# \"video_id\", \"frame\", \"polygon_label\", and either \"points\" or \"interp_polygon\" for the polygon coordinates.\n",
    "\n",
    "# Group the DataFrame by 'video_id' and 'polygon_label', then apply the track assignment function to each group.\n",
    "df_polygons = (\n",
    "    df_polygons.groupby([\"video_id\", \"polygon_label\"], group_keys=False)\n",
    "             .apply(lambda g: assign_tracks_poly(g, max_distance=50))\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# df_with_tracks now contains an additional column \"polygon_track_id\" that tracks polygon instances across frames.\n",
    "# df_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polygons interpolation\n",
    "The program addresses a common challenge in video annotation: manually labeling every frame is extremely time-consuming. Typically, experts annotate only every 10th frame (or another interval), which means many intermediate frames lack direct annotations. To solve this, the program includes an interpolation module that calculates the polygon annotations for these in-between frames. By leveraging the annotations from the two nearest labeled frames, it estimates the positions and shapes of the polygons for all intervening frames, ensuring a complete and continuous labeling of the entire video sequence without requiring exhaustive manual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_points(polyA, polyB, t):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two polygons that each have the same number of vertices.\n",
    "    \n",
    "    Given two polygons, polyA and polyB, each represented as a list of (x, y) tuples, this function computes\n",
    "    an interpolated polygon by linearly blending each corresponding pair of vertices using the parameter t.\n",
    "    \n",
    "    The interpolation formula for each coordinate is:\n",
    "        interpolated_value = (1 - t) * value_A + t * value_B\n",
    "    where t is a fraction between 0 and 1:\n",
    "      - t = 0 yields polyA,\n",
    "      - t = 1 yields polyB,\n",
    "      - and values in between yield points along the straight line between the two vertices.\n",
    "    \n",
    "    Parameters:\n",
    "        polyA: list of (x, y) tuples representing the first polygon.\n",
    "        polyB: list of (x, y) tuples representing the second polygon.\n",
    "        t: float in [0, 1] representing the interpolation factor.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (x, y) tuples representing the interpolated polygon.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Iterate over corresponding vertices of both polygons\n",
    "    for (xA, yA), (xB, yB) in zip(polyA, polyB):\n",
    "        # Compute the linear interpolation for x and y separately.\n",
    "        x = (1 - t) * xA + t * xB\n",
    "        y = (1 - t) * yA + t * yB\n",
    "        result.append((x, y))\n",
    "    return result\n",
    "\n",
    "def interpolate_polygon_track(df_track):\n",
    "    \"\"\"\n",
    "    Interpolate polygon annotations over a track across consecutive frames.\n",
    "    \n",
    "    For a single track—identified by a unique combination of video_id and polygon_id—this function:\n",
    "      - Sorts the annotations by frame.\n",
    "      - Creates a complete DataFrame that includes every frame from the minimum to the maximum annotated frame.\n",
    "      - For each consecutive pair of annotated frames, it linearly interpolates the polygon points for \n",
    "        the frames in between using linear interpolation.\n",
    "      - Copies constant identification columns (such as video_id, polygon_id, name_cvat, polygon_label, type)\n",
    "        from the original annotations to the interpolated frames.\n",
    "      \n",
    "    The polygon is assumed to be stored in the \"points\" column, and the resulting interpolated polygon\n",
    "    is stored in a new column \"interp_polygon\".\n",
    "    \n",
    "    Mathematical Details:\n",
    "      - For two annotated frames f_start and f_end with known polygons poly_start and poly_end, the gap is defined\n",
    "        as (f_end - f_start). For each intermediate frame at an offset 'offset' from f_start, the interpolation \n",
    "        factor t is calculated as t = offset / gap.\n",
    "      - The function then uses the interpolate_points function to calculate the polygon for that frame.\n",
    "    \n",
    "    Parameters:\n",
    "        df_track (pd.DataFrame): DataFrame for a single polygon track with annotations.\n",
    "        max_distance: (not used in this function, but kept for consistency with overall pipeline).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an \"interp_polygon\" column containing interpolated polygon points for\n",
    "                      every frame in the track.\n",
    "    \"\"\"\n",
    "    # Sort the track by frame to maintain temporal order.\n",
    "    df_track = df_track.sort_values(\"frame\").reset_index(drop=True)\n",
    "    \n",
    "    # Create a DataFrame with one row per frame between the minimum and maximum annotated frames.\n",
    "    min_frame = df_track[\"frame\"].min()\n",
    "    max_frame = df_track[\"frame\"].max()\n",
    "    all_frames = pd.DataFrame({\"frame\": range(min_frame, max_frame + 1)})\n",
    "    \n",
    "    # Merge to ensure that annotated frames retain their original data.\n",
    "    merged = pd.merge(all_frames, df_track, on=\"frame\", how=\"left\")\n",
    "    \n",
    "    # Initialize a column to store interpolated polygon data.\n",
    "    merged[\"interp_polygon\"] = None\n",
    "    \n",
    "    # Identify the indices of rows that already have polygon annotations (i.e., \"points\" are available).\n",
    "    annotated_idx = merged[~merged[\"points\"].isna()].index\n",
    "    \n",
    "    # Process each pair of consecutive annotated frames.\n",
    "    for start_i, end_i in zip(annotated_idx, annotated_idx[1:]):\n",
    "        # Retrieve the frame numbers for the start and end of the current segment.\n",
    "        f_start = merged.loc[start_i, \"frame\"]\n",
    "        f_end = merged.loc[end_i, \"frame\"]\n",
    "        \n",
    "        # Get the polygon points from the start and end frames.\n",
    "        poly_start = merged.loc[start_i, \"points\"]  # Expected to be a list of (x,y) tuples.\n",
    "        poly_end = merged.loc[end_i, \"points\"]\n",
    "        \n",
    "        # Calculate the number of frames between the two annotated frames.\n",
    "        gap = f_end - f_start\n",
    "        \n",
    "        # For each frame in the gap, compute the interpolation factor t and the corresponding polygon.\n",
    "        for offset in range(gap + 1):\n",
    "            # If gap is 0 (shouldn't happen, but for safety), t defaults to 0.\n",
    "            t = offset / float(gap) if gap else 0\n",
    "            # Compute the interpolated polygon using linear interpolation on each vertex.\n",
    "            poly_interp = interpolate_points(poly_start, poly_end, t)\n",
    "            # Store the interpolated polygon in the corresponding row.\n",
    "            merged.at[start_i + offset, \"interp_polygon\"] = poly_interp\n",
    "    \n",
    "    # For frames that were originally annotated, ensure that interp_polygon matches the original points.\n",
    "    merged.loc[annotated_idx, \"interp_polygon\"] = merged.loc[annotated_idx, \"points\"]\n",
    "    \n",
    "    # Propagate constant identification columns from the original annotations to every row.\n",
    "    for col in [\"video_id\", \"polygon_id\", \"name_cvat\", \"polygon_label\", \"type\"]:\n",
    "        if col in df_track.columns:\n",
    "            merged[col] = df_track[col].iloc[0]\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def interpolate_all_polygons(df, num_points=None):\n",
    "    \"\"\"\n",
    "    Interpolate polygon annotations for all polygon tracks in a DataFrame.\n",
    "    \n",
    "    This function processes a DataFrame that contains polygon annotations with columns such as\n",
    "    video_id, polygon_id, frame, and points. It groups the DataFrame by video_id and polygon_track_id,\n",
    "    and then applies interpolation for each group. The result is a DataFrame that contains an interpolated\n",
    "    polygon (\"interp_polygon\") for every frame in the range of each polygon track.\n",
    "    \n",
    "    Note:\n",
    "      - It assumes that the polygon points in \"points\" have the same number of vertices across annotations.\n",
    "        If they do not, a resampling step might be necessary prior to interpolation.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing polygon annotations.\n",
    "        num_points (optional): Parameter reserved for potential resampling of polygon vertices (not used here).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with interpolated polygon annotations for each frame.\n",
    "    \"\"\"\n",
    "    df_result = (\n",
    "        df.groupby([\"video_id\", \"polygon_track_id\"], group_keys=True)\n",
    "          .apply(interpolate_polygon_track)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polygons count before interpolation -> 3198\n",
      "Polygons count before interpolation -> 22445\n"
     ]
    }
   ],
   "source": [
    "print(f'Polygons count before interpolation -> {df_polygons.shape[0]}')\n",
    "df_polygons = interpolate_all_polygons(df_polygons)  # filter for a given video)\n",
    "print(f'Polygons count before interpolation -> {df_polygons.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sliding absent and present to lung point pleura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = df_videos.loc[df_videos[\"video_id\"].isin(set(df_polygons[\"video_id\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP -> 17\n",
      "A -> 9\n",
      "P -> 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>name_cvat</th>\n",
       "      <th>name_video</th>\n",
       "      <th>video_subfolder_path</th>\n",
       "      <th>task_status</th>\n",
       "      <th>folder_class</th>\n",
       "      <th>video_resolution_x</th>\n",
       "      <th>video_resolution_y</th>\n",
       "      <th>length</th>\n",
       "      <th>task_id</th>\n",
       "      <th>fps</th>\n",
       "      <th>project_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>USG - Lung point/task_261</td>\n",
       "      <td>annotation</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>999.0</td>\n",
       "      <td>task_261</td>\n",
       "      <td>54.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>IVIXFY95</td>\n",
       "      <td>x_20180621_161347_1853 LPNO LUNG POINT.avi</td>\n",
       "      <td>20180621_161347_1853 LPNO LUNG POINT.avi</td>\n",
       "      <td>USG - Lung point/task_262</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>999.0</td>\n",
       "      <td>task_262</td>\n",
       "      <td>54.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1AY9XWWL</td>\n",
       "      <td>014_image_67666469659947.mp4</td>\n",
       "      <td>image_67666469659947.mp4</td>\n",
       "      <td>USG - Lung point/task_266</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "      <td>299.0</td>\n",
       "      <td>task_266</td>\n",
       "      <td>30.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>DE3IOJXQ</td>\n",
       "      <td>x211027--111948_20211027_MSK_0011lp.AVI</td>\n",
       "      <td>x211027--111948_20211027_MSK_0011lp.AVI</td>\n",
       "      <td>USG - Lung point/task_267</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_267</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Q8LX0Y1B</td>\n",
       "      <td>x211027--111948_20211027_MSK_0012lp.AVI</td>\n",
       "      <td>x211027--111948_20211027_MSK_0012lp.AVI</td>\n",
       "      <td>USG - Lung point/task_268</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_268</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>SH8SNPMW</td>\n",
       "      <td>x211119--131441_20211119_MSK_0001 LPAB.AVI</td>\n",
       "      <td>x211119--131441_20211119_MSK_0001 LPAB.AVI</td>\n",
       "      <td>USG - Lung point/task_269</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_269</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>GHOX1FZE</td>\n",
       "      <td>x211119--131441_20211119_MSK_0002LPB.AVI</td>\n",
       "      <td>x211119--131441_20211119_MSK_0002LPB.AVI</td>\n",
       "      <td>USG - Lung point/task_270</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_270</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Q1Z8FQL0</td>\n",
       "      <td>x211119--131441_20211119_MSK_0003LPAB.AVI</td>\n",
       "      <td>x211119--131441_20211119_MSK_0003LPAB.AVI</td>\n",
       "      <td>USG - Lung point/task_271</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>294.0</td>\n",
       "      <td>task_271</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0CJ4LA0L</td>\n",
       "      <td>x211119--131441_20211119_MSK_0004 LPB.AVI</td>\n",
       "      <td>x211119--131441_20211119_MSK_0004 LPB.AVI</td>\n",
       "      <td>USG - Lung point/task_272</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_272</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>YOBBHM8M</td>\n",
       "      <td>x220425--091211_20220425_MSK_0004lp.AVI</td>\n",
       "      <td>x220425--091211_20220425_MSK_0004lp.AVI</td>\n",
       "      <td>USG - Lung point/task_273</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_273</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>BL506TPN</td>\n",
       "      <td>x220425--091211_20220425_MSK_0005lpq.AVI</td>\n",
       "      <td>x220425--091211_20220425_MSK_0005lpq.AVI</td>\n",
       "      <td>USG - Lung point/task_274</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_274</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>DC8PMFR4</td>\n",
       "      <td>x220425--091211_20220425_MSK_0011lp.AVI</td>\n",
       "      <td>x220425--091211_20220425_MSK_0011lp.AVI</td>\n",
       "      <td>USG - Lung point/task_275</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_275</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>N1F4EAHY</td>\n",
       "      <td>x220427--090640_20220427_MSK_0005lp.AVI</td>\n",
       "      <td>x220427--090640_20220427_MSK_0005lp.AVI</td>\n",
       "      <td>USG - Lung point/task_276</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_276</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1LJHCXSV</td>\n",
       "      <td>x220427--090640_20220427_MSK_0006lp.AVI</td>\n",
       "      <td>x220427--090640_20220427_MSK_0006lp.AVI</td>\n",
       "      <td>USG - Lung point/task_277</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_277</td>\n",
       "      <td>30.303</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>3JXEFA01</td>\n",
       "      <td>020_image_154606543088800.mp4</td>\n",
       "      <td>image_154606543088800.mp4</td>\n",
       "      <td>USG - Lung point/task_426</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>3840</td>\n",
       "      <td>2160</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_426</td>\n",
       "      <td>30.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>4JTDEJKT</td>\n",
       "      <td>023_image_183378630848567.mp4</td>\n",
       "      <td>image_183378630848567.mp4</td>\n",
       "      <td>USG - Lung point/task_427</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>3840</td>\n",
       "      <td>2160</td>\n",
       "      <td>299.0</td>\n",
       "      <td>task_427</td>\n",
       "      <td>30.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>EKONFXRF</td>\n",
       "      <td>064_image_3324193904814.mp4</td>\n",
       "      <td>image_3324193904814.mp4</td>\n",
       "      <td>USG - Lung point/task_428</td>\n",
       "      <td>completed</td>\n",
       "      <td>USG - Lung point</td>\n",
       "      <td>3840</td>\n",
       "      <td>2160</td>\n",
       "      <td>300.0</td>\n",
       "      <td>task_428</td>\n",
       "      <td>30.000</td>\n",
       "      <td>lung_poing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id                                   name_cvat  \\\n",
       "187  PFFP3KRO               014 2021-10-18_125058_159.avi   \n",
       "188  IVIXFY95  x_20180621_161347_1853 LPNO LUNG POINT.avi   \n",
       "189  1AY9XWWL                014_image_67666469659947.mp4   \n",
       "190  DE3IOJXQ     x211027--111948_20211027_MSK_0011lp.AVI   \n",
       "191  Q8LX0Y1B     x211027--111948_20211027_MSK_0012lp.AVI   \n",
       "192  SH8SNPMW  x211119--131441_20211119_MSK_0001 LPAB.AVI   \n",
       "193  GHOX1FZE    x211119--131441_20211119_MSK_0002LPB.AVI   \n",
       "194  Q1Z8FQL0   x211119--131441_20211119_MSK_0003LPAB.AVI   \n",
       "195  0CJ4LA0L   x211119--131441_20211119_MSK_0004 LPB.AVI   \n",
       "196  YOBBHM8M     x220425--091211_20220425_MSK_0004lp.AVI   \n",
       "197  BL506TPN    x220425--091211_20220425_MSK_0005lpq.AVI   \n",
       "198  DC8PMFR4     x220425--091211_20220425_MSK_0011lp.AVI   \n",
       "199  N1F4EAHY     x220427--090640_20220427_MSK_0005lp.AVI   \n",
       "200  1LJHCXSV     x220427--090640_20220427_MSK_0006lp.AVI   \n",
       "201  3JXEFA01               020_image_154606543088800.mp4   \n",
       "202  4JTDEJKT               023_image_183378630848567.mp4   \n",
       "203  EKONFXRF                 064_image_3324193904814.mp4   \n",
       "\n",
       "                                     name_video       video_subfolder_path  \\\n",
       "187               014 2021-10-18_125058_159.avi  USG - Lung point/task_261   \n",
       "188    20180621_161347_1853 LPNO LUNG POINT.avi  USG - Lung point/task_262   \n",
       "189                    image_67666469659947.mp4  USG - Lung point/task_266   \n",
       "190     x211027--111948_20211027_MSK_0011lp.AVI  USG - Lung point/task_267   \n",
       "191     x211027--111948_20211027_MSK_0012lp.AVI  USG - Lung point/task_268   \n",
       "192  x211119--131441_20211119_MSK_0001 LPAB.AVI  USG - Lung point/task_269   \n",
       "193    x211119--131441_20211119_MSK_0002LPB.AVI  USG - Lung point/task_270   \n",
       "194   x211119--131441_20211119_MSK_0003LPAB.AVI  USG - Lung point/task_271   \n",
       "195   x211119--131441_20211119_MSK_0004 LPB.AVI  USG - Lung point/task_272   \n",
       "196     x220425--091211_20220425_MSK_0004lp.AVI  USG - Lung point/task_273   \n",
       "197    x220425--091211_20220425_MSK_0005lpq.AVI  USG - Lung point/task_274   \n",
       "198     x220425--091211_20220425_MSK_0011lp.AVI  USG - Lung point/task_275   \n",
       "199     x220427--090640_20220427_MSK_0005lp.AVI  USG - Lung point/task_276   \n",
       "200     x220427--090640_20220427_MSK_0006lp.AVI  USG - Lung point/task_277   \n",
       "201                   image_154606543088800.mp4  USG - Lung point/task_426   \n",
       "202                   image_183378630848567.mp4  USG - Lung point/task_427   \n",
       "203                     image_3324193904814.mp4  USG - Lung point/task_428   \n",
       "\n",
       "    task_status      folder_class video_resolution_x video_resolution_y  \\\n",
       "187  annotation  USG - Lung point                640                480   \n",
       "188   completed  USG - Lung point                640                480   \n",
       "189   completed  USG - Lung point               1280                720   \n",
       "190   completed  USG - Lung point                800                600   \n",
       "191   completed  USG - Lung point                800                600   \n",
       "192   completed  USG - Lung point                800                600   \n",
       "193   completed  USG - Lung point                800                600   \n",
       "194   completed  USG - Lung point                800                600   \n",
       "195   completed  USG - Lung point                800                600   \n",
       "196   completed  USG - Lung point                800                600   \n",
       "197   completed  USG - Lung point                800                600   \n",
       "198   completed  USG - Lung point                800                600   \n",
       "199   completed  USG - Lung point                800                600   \n",
       "200   completed  USG - Lung point                800                600   \n",
       "201   completed  USG - Lung point               3840               2160   \n",
       "202   completed  USG - Lung point               3840               2160   \n",
       "203   completed  USG - Lung point               3840               2160   \n",
       "\n",
       "     length   task_id     fps project_class  \n",
       "187   999.0  task_261  54.000    lung_poing  \n",
       "188   999.0  task_262  54.000    lung_poing  \n",
       "189   299.0  task_266  30.000    lung_poing  \n",
       "190   300.0  task_267  30.303    lung_poing  \n",
       "191   300.0  task_268  30.303    lung_poing  \n",
       "192   300.0  task_269  30.303    lung_poing  \n",
       "193   300.0  task_270  30.303    lung_poing  \n",
       "194   294.0  task_271  30.303    lung_poing  \n",
       "195   300.0  task_272  30.303    lung_poing  \n",
       "196   300.0  task_273  30.303    lung_poing  \n",
       "197   300.0  task_274  30.303    lung_poing  \n",
       "198   300.0  task_275  30.303    lung_poing  \n",
       "199   300.0  task_276  30.303    lung_poing  \n",
       "200   300.0  task_277  30.303    lung_poing  \n",
       "201   300.0  task_426  30.000    lung_poing  \n",
       "202   299.0  task_427  30.000    lung_poing  \n",
       "203   300.0  task_428  30.000    lung_poing  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dffff = df_videos.loc[]\n",
    "\n",
    "print(f'LP -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung point\"].shape[0]}')\n",
    "print(f'A -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung sliding absent (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "print(f'P -> {dffff.loc[dffff[\"folder_class\"] == \"USG - Lung sliding present (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "\n",
    "17 + 9 + 26\n",
    "\n",
    "dffff.loc[df_check[\"folder_class\"] == \"USG - Lung point\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh731nk_apvv-lung-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
