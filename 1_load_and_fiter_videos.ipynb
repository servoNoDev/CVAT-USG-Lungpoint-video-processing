{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import random as rand\n",
    "import ast\n",
    "from typing import Any, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVAT_FOLDER_PATH = \"/home/mh731nk/_data/experiments_tmp/data/revision_8/cvat_project_raw_unzip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read video DataFrame \n",
    "Data was prepared in the previous part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "df_videos = pd.read_pickle(\"/home/mh731nk/_data/experiments_tmp/data/revision_8/video.pkl\", 'zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select videos for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LP -> 17\n",
      "A -> 9\n",
      "P -> 26\n"
     ]
    }
   ],
   "source": [
    "df_filtere_videos = df_videos.loc[df_videos[\"folder_class\"].isin([\n",
    "    'USG - Lung point',\n",
    "    'USG - Lung sliding absent (aj ine znaky - radiol. klin.)',\n",
    "    'USG - Lung sliding present (aj ine znaky - radiol. klin.)'\n",
    "    ])]\n",
    "\n",
    "print(f'LP -> {df_filtere_videos.loc[df_videos[\"folder_class\"] == \"USG - Lung point\"].shape[0]}')\n",
    "print(f'A -> {df_filtere_videos.loc[df_videos[\"folder_class\"] == \"USG - Lung sliding absent (aj ine znaky - radiol. klin.)\"].shape[0]}')\n",
    "print(f'P -> {df_filtere_videos.loc[df_videos[\"folder_class\"] == \"USG - Lung sliding present (aj ine znaky - radiol. klin.)\"].shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read anotations for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_generator(self, size: int = 8, chars: str = string.ascii_uppercase + string.digits) -> str:\n",
    "    \"\"\"\n",
    "    Generate a random string identifier.\n",
    "\n",
    "    This method generates a random string of the specified size using a secure random\n",
    "    generator. By default, the generated string consists of uppercase letters and digits.\n",
    "\n",
    "    Args:\n",
    "        size (int): The length of the generated string. Defaults to 8.\n",
    "        chars (str): A string containing the set of characters to choose from.\n",
    "                     Defaults to uppercase letters and digits (A-Z, 0-9).\n",
    "\n",
    "    Returns:\n",
    "        str: A randomly generated string identifier.\n",
    "    \"\"\"\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "\n",
    "def parse_polygon_points(pts: Union[str, List[Any]]) -> List[Tuple[Union[int, float], Union[int, float]]]:\n",
    "    \"\"\"\n",
    "    Parse and normalize polygon points into a list of (x, y) coordinate pairs.\n",
    "    \n",
    "    The input can be provided in different formats:\n",
    "    - A string representation of a list (e.g., \"[556.9, 200.3, 555.3, 214.0, ...]\"), \n",
    "      which will be evaluated into a Python list.\n",
    "    - A flat list of numbers representing coordinates (e.g., [x0, y0, x1, y1, ...]).\n",
    "    - A list already containing (x, y) pairs.\n",
    "    \n",
    "    The function ensures that the returned value is always a list of tuples,\n",
    "    where each tuple represents a point as (x, y).\n",
    "    \n",
    "    Args:\n",
    "        pts (Union[str, List[Any]]): The polygon points in one of the accepted formats.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[Union[int, float], Union[int, float]]]: A list of (x, y) coordinate pairs.\n",
    "    \"\"\"\n",
    "    # If pts is a string, safely evaluate it to convert the string into a Python list.\n",
    "    if isinstance(pts, str):\n",
    "        pts = ast.literal_eval(pts)\n",
    "    \n",
    "    # If the list is a flat list of coordinates (e.g., [x0, y0, x1, y1, ...]),\n",
    "    # then group the numbers into (x, y) pairs.\n",
    "    if pts and isinstance(pts[0], (float, int)):\n",
    "        pts = list(zip(pts[::2], pts[1::2]))\n",
    "    \n",
    "    # Return the normalized list of (x, y) coordinate pairs.\n",
    "    return pts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each video exported from the CVAT annotation tool is organized into a structured folder that holds both the raw video data and its associated metadata. This structure typically includes:\n",
    "\n",
    "Raw Video Storage:\n",
    "The primary video file is saved in a dedicated folder, ensuring that the original footage is preserved intact.\n",
    "\n",
    "Metadata Files:\n",
    "Accompanying the raw video are metadata files (often in JSON format) that provide detailed information about the video, such as resolution, frame rate, and other relevant properties.\n",
    "\n",
    "Annotation Data:\n",
    "In addition to video metadata, the export includes detailed annotation files. These files contain information about label masks and polygon coordinates for each frame, which outline the annotated regions of interest within the video. This structured annotation data is essential for tasks like object detection and segmentation.\n",
    "\n",
    "This systematic folder organization makes it easy to access and process both the video content and its annotations, streamlining workflows for analysis and machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing of 52 videos\n",
      "Dataset include 2658 polygons.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold rows extracted from annotations\n",
    "rows_to_append = []\n",
    "print(f\"Start processing of {df_filtere_videos.shape[0]} videos\")\n",
    "# Iterate over each row in the DataFrame containing filtered videos\n",
    "for index, video_df_row in df_filtere_videos.iterrows():\n",
    "    \n",
    "    # Build the path to the 'data' folder within the current video's subfolder\n",
    "    data_folder_path = os.path.join(\n",
    "        CVAT_FOLDER_PATH, \n",
    "        video_df_row[\"video_subfolder_path\"], \n",
    "        'data'\n",
    "    )\n",
    "    \n",
    "    # Walk through the directory tree starting at data_folder_path.\n",
    "    # os.walk returns tuples of (current_path, directories, files)\n",
    "    folders_scan = [x for x in os.walk(data_folder_path)]\n",
    "    \n",
    "    # Retrieve the first subdirectory from the scan (assumes at least one exists)\n",
    "    video_folder = folders_scan[0][1][0]\n",
    "    \n",
    "    # Print the name of the video folder (for debugging or logging purposes)\n",
    "    # print(f' Video {video_df_row[\"video_id\"]} is processing')\n",
    "    \n",
    "    # Construct the full path to the video subfolder\n",
    "    path_video = os.path.join(\n",
    "        CVAT_FOLDER_PATH,\n",
    "        video_df_row[\"video_subfolder_path\"]\n",
    "    )\n",
    "    \n",
    "    # Open and load the annotations JSON file\n",
    "    with open(os.path.join(path_video, 'annotations.json')) as json_file:\n",
    "        json_anotation = json.load(json_file)\n",
    "    \n",
    "    # Open and load the task JSON file\n",
    "    with open(os.path.join(path_video, 'task.json')) as json_file:\n",
    "        task = json.load(json_file)\n",
    "    \n",
    "    # Iterate over each polygon shape in the first annotation object\n",
    "    for polygon in json_anotation[0]['shapes']:\n",
    "        # Create a dictionary with the extracted details for each polygon\n",
    "        row = {\n",
    "            'polygon_id': id_generator(8),               # Generate a unique identifier for the polygon\n",
    "            'video_id': video_df_row[\"video_id\"],        # Retrieve the video ID from the DataFrame row\n",
    "            'name_cvat': task[\"name\"],                   # Retrieve the task name from the task JSON\n",
    "            'type': polygon[\"type\"],                     # The type of shape (e.g., polygon, rectangle)\n",
    "            'frame': int(polygon[\"frame\"]),              # Convert the frame number to an integer\n",
    "            'polygon_label': polygon[\"label\"],           # The label assigned to the polygon (e.g., object class)\n",
    "            'points': parse_polygon_points(list(polygon[\"points\"]))            # List of coordinate points defining the polygon -> need to be parsed\n",
    "        }\n",
    "        # Append the dictionary to the list of rows to be processed later\n",
    "        rows_to_append.append(row)\n",
    "\n",
    "df_polygons = pd.DataFrame(rows_to_append) \n",
    "print(f'Dataset include {df_polygons.shape[0]} polygons.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>name_cvat</th>\n",
       "      <th>type</th>\n",
       "      <th>frame</th>\n",
       "      <th>polygon_label</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NQQPLYA2</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>0</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.9000000000015, 200.3000000000011), (555....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DH51BLCW</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>0</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(359.705078125, 186.00537109375), (359.704868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U71I1HCF</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>10</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(366.041015625, 185.21337890625), (366.040805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AY4KUJ37</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>10</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.9000000000015, 201.8000000000011), (555....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7LNRW3JT</td>\n",
       "      <td>PFFP3KRO</td>\n",
       "      <td>014 2021-10-18_125058_159.avi</td>\n",
       "      <td>polygon</td>\n",
       "      <td>20</td>\n",
       "      <td>lungslidingabsent</td>\n",
       "      <td>[(556.1000000000004, 207.40000000000146), (555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>HQX9XFFI</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>260</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(365.4625976562511, 203.77187500000036), (361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2654</th>\n",
       "      <td>5PB118QJ</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>270</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1701171875011, 203.13808593750036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>WTTEZCYQ</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>280</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1701171875011, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>I7LNNXTU</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>290</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1706054687511, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>41MYB0GL</td>\n",
       "      <td>T7EUVCFF</td>\n",
       "      <td>x220819--071841_20220819_MSK_0005s.AVI</td>\n",
       "      <td>polygon</td>\n",
       "      <td>299</td>\n",
       "      <td>lungslidingpresent</td>\n",
       "      <td>[(371.1706054687511, 202.50332031250036), (366...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2658 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polygon_id  video_id                               name_cvat     type  \\\n",
       "0      NQQPLYA2  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "1      DH51BLCW  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "2      U71I1HCF  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "3      AY4KUJ37  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "4      7LNRW3JT  PFFP3KRO           014 2021-10-18_125058_159.avi  polygon   \n",
       "...         ...       ...                                     ...      ...   \n",
       "2653   HQX9XFFI  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "2654   5PB118QJ  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "2655   WTTEZCYQ  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "2656   I7LNNXTU  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "2657   41MYB0GL  T7EUVCFF  x220819--071841_20220819_MSK_0005s.AVI  polygon   \n",
       "\n",
       "      frame       polygon_label  \\\n",
       "0         0   lungslidingabsent   \n",
       "1         0   lungslidingabsent   \n",
       "2        10   lungslidingabsent   \n",
       "3        10   lungslidingabsent   \n",
       "4        20   lungslidingabsent   \n",
       "...     ...                 ...   \n",
       "2653    260  lungslidingpresent   \n",
       "2654    270  lungslidingpresent   \n",
       "2655    280  lungslidingpresent   \n",
       "2656    290  lungslidingpresent   \n",
       "2657    299  lungslidingpresent   \n",
       "\n",
       "                                                 points  \n",
       "0     [(556.9000000000015, 200.3000000000011), (555....  \n",
       "1     [(359.705078125, 186.00537109375), (359.704868...  \n",
       "2     [(366.041015625, 185.21337890625), (366.040805...  \n",
       "3     [(556.9000000000015, 201.8000000000011), (555....  \n",
       "4     [(556.1000000000004, 207.40000000000146), (555...  \n",
       "...                                                 ...  \n",
       "2653  [(365.4625976562511, 203.77187500000036), (361...  \n",
       "2654  [(371.1701171875011, 203.13808593750036), (366...  \n",
       "2655  [(371.1701171875011, 202.50332031250036), (366...  \n",
       "2656  [(371.1706054687511, 202.50332031250036), (366...  \n",
       "2657  [(371.1706054687511, 202.50332031250036), (366...  \n",
       "\n",
       "[2658 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an instance ID for each polygon annotation.\n",
    "\n",
    "In a single frame, multiple polygons can be annotated, potentially with different labels. Additionally, it is possible\n",
    "to have multiple polygons of the same type within the same frame. In the original dataset, polygons corresponding to the\n",
    "same observed object across different frames do not share a common reference. Assigning a unique instance ID to each\n",
    "polygon is crucial for later interpolation operations, as it allows for linking and tracking the same object across frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import List, Tuple, Union, Any\n",
    "\n",
    "def compute_polygon_centroid(poly: List[Tuple[Union[int, float], Union[int, float]]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the centroid of a polygon represented by a list of (x, y) tuples.\n",
    "    \n",
    "    The centroid is calculated as the arithmetic mean of the vertices.\n",
    "    \n",
    "    Parameters:\n",
    "        poly (List[Tuple[Union[int, float], Union[int, float]]]):\n",
    "            A list of (x, y) tuples representing the polygon's vertices.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A numpy array containing the (x, y) coordinates of the centroid.\n",
    "    \"\"\"\n",
    "    # Convert the list of points into a numpy array for efficient computation.\n",
    "    pts = np.array(poly)\n",
    "    # Calculate the mean of x and y coordinates separately and return as a numpy array.\n",
    "    return np.array([pts[:, 0].mean(), pts[:, 1].mean()])\n",
    "\n",
    "\n",
    "def get_polygon_from_row(row: Any) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve polygon data from a row of a DataFrame.\n",
    "    \n",
    "    This function checks for polygon data in two possible columns:\n",
    "      - 'interp_polygon': preferred if available.\n",
    "      - 'points': used if 'interp_polygon' is not available.\n",
    "    \n",
    "    Parameters:\n",
    "        row (Any): A dictionary-like object (e.g., a pandas Series) representing a row in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        List: A list representing the polygon points (assumed to be (x, y) tuples).\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If neither 'interp_polygon' nor 'points' is found in the row.\n",
    "    \"\"\"\n",
    "    # Check for interpolated polygon data first.\n",
    "    poly = row.get('interp_polygon', None)\n",
    "    if poly is not None:\n",
    "        return poly\n",
    "    # Fallback to original points if interpolated data is not available.\n",
    "    poly = row.get('points', None)\n",
    "    if poly is not None:\n",
    "        return poly\n",
    "    # Raise an error if no polygon data is found.\n",
    "    raise ValueError(\"Row does not contain polygon data.\")\n",
    "\n",
    "\n",
    "def assign_tracks_poly(df_group: pd.DataFrame, max_distance: float = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign consistent track IDs to polygon annotations across frames.\n",
    "    \n",
    "    For a given group (annotations from the same video and with the same polygon label),\n",
    "    this function assigns a unique \"polygon_track_id\" to each polygon such that the same\n",
    "    object is tracked across consecutive frames. This is achieved by comparing the centroids\n",
    "    of polygons between frames using the Hungarian algorithm to minimize the assignment cost.\n",
    "    \n",
    "    Parameters:\n",
    "        df_group (pd.DataFrame):\n",
    "            DataFrame containing polygon annotations for a single video and a specific polygon label.\n",
    "            Expected columns include \"frame\" and either \"interp_polygon\" or \"points\" for polygon data.\n",
    "        max_distance (float):\n",
    "            The maximum Euclidean distance allowed for linking two polygons across frames.\n",
    "            Polygons with a centroid distance exceeding this value are not considered the same object.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            A new DataFrame with an added \"polygon_track_id\" column that provides the track IDs.\n",
    "    \"\"\"\n",
    "    # Sort the group by frame to ensure temporal order.\n",
    "    df_group = df_group.sort_values(\"frame\").reset_index(drop=True)\n",
    "    \n",
    "    # Initialize the 'polygon_track_id' column with a default value of -1 (unassigned).\n",
    "    df_group[\"polygon_track_id\"] = -1\n",
    "    next_track_id = 0  # Counter for assigning new track IDs.\n",
    "\n",
    "    # List to store information about polygons from the previous frame.\n",
    "    # Each element is a dictionary with keys: \"track_id\" and \"centroid\".\n",
    "    prev_tracks = []\n",
    "\n",
    "    # Array to store the new track IDs corresponding to each row.\n",
    "    new_ids = np.empty(len(df_group), dtype=int)\n",
    "    \n",
    "    # Process the DataFrame frame by frame.\n",
    "    for frame, frame_df in df_group.groupby(\"frame\"):\n",
    "        # Get the indices of rows in the current frame.\n",
    "        idx = frame_df.index.tolist()\n",
    "        # List to store the computed centroids for polygons in the current frame.\n",
    "        centroids = []\n",
    "        for i in idx:\n",
    "            # Retrieve polygon data from the current row.\n",
    "            poly = get_polygon_from_row(df_group.loc[i])\n",
    "            # Compute the centroid for the polygon.\n",
    "            centroid = compute_polygon_centroid(poly)\n",
    "            centroids.append(centroid)\n",
    "        \n",
    "        # Initialize an array for storing track IDs for polygons in the current frame.\n",
    "        frame_ids = np.full(len(idx), -1, dtype=int)\n",
    "        \n",
    "        if not prev_tracks:\n",
    "            # For the first frame, assign a new track ID to every polygon.\n",
    "            for j in range(len(idx)):\n",
    "                frame_ids[j] = next_track_id\n",
    "                next_track_id += 1\n",
    "        else:\n",
    "            # Build a cost matrix where each entry is the Euclidean distance between\n",
    "            # a polygon in the previous frame and one in the current frame.\n",
    "            cost_matrix = np.zeros((len(prev_tracks), len(idx)))\n",
    "            for i, prev in enumerate(prev_tracks):\n",
    "                for j, current_centroid in enumerate(centroids):\n",
    "                    cost_matrix[i, j] = np.linalg.norm(prev[\"centroid\"] - current_centroid)\n",
    "            \n",
    "            # Use the Hungarian algorithm to determine the best assignment between previous and current polygons.\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            # For each assignment, if the distance is within the allowed max_distance,\n",
    "            # assign the same track ID from the previous frame.\n",
    "            for r, c in zip(row_ind, col_ind):\n",
    "                if cost_matrix[r, c] <= max_distance:\n",
    "                    frame_ids[c] = prev_tracks[r][\"track_id\"]\n",
    "            \n",
    "            # For any polygon that did not receive a track ID, assign a new one.\n",
    "            for j in range(len(idx)):\n",
    "                if frame_ids[j] == -1:\n",
    "                    frame_ids[j] = next_track_id\n",
    "                    next_track_id += 1\n",
    "        \n",
    "        # Record the assigned track IDs for the current frame in the new_ids array.\n",
    "        for k, i in enumerate(idx):\n",
    "            new_ids[i] = frame_ids[k]\n",
    "        \n",
    "        # Update the prev_tracks list with polygons from the current frame for use in the next iteration.\n",
    "        prev_tracks = []\n",
    "        for j, i in enumerate(idx):\n",
    "            prev_tracks.append({\n",
    "                \"track_id\": frame_ids[j],\n",
    "                \"centroid\": centroids[j]\n",
    "            })\n",
    "    \n",
    "    # Assign the computed track IDs back to the DataFrame.\n",
    "    df_group[\"polygon_track_id\"] = new_ids\n",
    "    return df_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Example Usage -----\n",
    "# Assume df_polygons_revision_9 is a DataFrame containing polygon annotations with the following columns:\n",
    "# \"video_id\", \"frame\", \"polygon_label\", and either \"points\" or \"interp_polygon\" for the polygon coordinates.\n",
    "\n",
    "# Group the DataFrame by 'video_id' and 'polygon_label', then apply the track assignment function to each group.\n",
    "df_polygons = (\n",
    "    df_polygons.groupby([\"video_id\", \"polygon_label\"], group_keys=False)\n",
    "             .apply(lambda g: assign_tracks_poly(g, max_distance=50))\n",
    "             .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# df_with_tracks now contains an additional column \"polygon_track_id\" that tracks polygon instances across frames.\n",
    "# df_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polygons interpolation\n",
    "The program addresses a common challenge in video annotation: manually labeling every frame is extremely time-consuming. Typically, experts annotate only every 10th frame (or another interval), which means many intermediate frames lack direct annotations. To solve this, the program includes an interpolation module that calculates the polygon annotations for these in-between frames. By leveraging the annotations from the two nearest labeled frames, it estimates the positions and shapes of the polygons for all intervening frames, ensuring a complete and continuous labeling of the entire video sequence without requiring exhaustive manual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_points(polyA, polyB, t):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two polygons that each have the same number of vertices.\n",
    "    \n",
    "    Given two polygons, polyA and polyB, each represented as a list of (x, y) tuples, this function computes\n",
    "    an interpolated polygon by linearly blending each corresponding pair of vertices using the parameter t.\n",
    "    \n",
    "    The interpolation formula for each coordinate is:\n",
    "        interpolated_value = (1 - t) * value_A + t * value_B\n",
    "    where t is a fraction between 0 and 1:\n",
    "      - t = 0 yields polyA,\n",
    "      - t = 1 yields polyB,\n",
    "      - and values in between yield points along the straight line between the two vertices.\n",
    "    \n",
    "    Parameters:\n",
    "        polyA: list of (x, y) tuples representing the first polygon.\n",
    "        polyB: list of (x, y) tuples representing the second polygon.\n",
    "        t: float in [0, 1] representing the interpolation factor.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (x, y) tuples representing the interpolated polygon.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # Iterate over corresponding vertices of both polygons\n",
    "    for (xA, yA), (xB, yB) in zip(polyA, polyB):\n",
    "        # Compute the linear interpolation for x and y separately.\n",
    "        x = (1 - t) * xA + t * xB\n",
    "        y = (1 - t) * yA + t * yB\n",
    "        result.append((x, y))\n",
    "    return result\n",
    "\n",
    "def interpolate_polygon_track(df_track):\n",
    "    \"\"\"\n",
    "    Interpolate polygon annotations over a track across consecutive frames.\n",
    "    \n",
    "    For a single track—identified by a unique combination of video_id and polygon_id—this function:\n",
    "      - Sorts the annotations by frame.\n",
    "      - Creates a complete DataFrame that includes every frame from the minimum to the maximum annotated frame.\n",
    "      - For each consecutive pair of annotated frames, it linearly interpolates the polygon points for \n",
    "        the frames in between using linear interpolation.\n",
    "      - Copies constant identification columns (such as video_id, polygon_id, name_cvat, polygon_label, type)\n",
    "        from the original annotations to the interpolated frames.\n",
    "      \n",
    "    The polygon is assumed to be stored in the \"points\" column, and the resulting interpolated polygon\n",
    "    is stored in a new column \"interp_polygon\".\n",
    "    \n",
    "    Mathematical Details:\n",
    "      - For two annotated frames f_start and f_end with known polygons poly_start and poly_end, the gap is defined\n",
    "        as (f_end - f_start). For each intermediate frame at an offset 'offset' from f_start, the interpolation \n",
    "        factor t is calculated as t = offset / gap.\n",
    "      - The function then uses the interpolate_points function to calculate the polygon for that frame.\n",
    "    \n",
    "    Parameters:\n",
    "        df_track (pd.DataFrame): DataFrame for a single polygon track with annotations.\n",
    "        max_distance: (not used in this function, but kept for consistency with overall pipeline).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with an \"interp_polygon\" column containing interpolated polygon points for\n",
    "                      every frame in the track.\n",
    "    \"\"\"\n",
    "    # Sort the track by frame to maintain temporal order.\n",
    "    df_track = df_track.sort_values(\"frame\").reset_index(drop=True)\n",
    "    \n",
    "    # Create a DataFrame with one row per frame between the minimum and maximum annotated frames.\n",
    "    min_frame = df_track[\"frame\"].min()\n",
    "    max_frame = df_track[\"frame\"].max()\n",
    "    all_frames = pd.DataFrame({\"frame\": range(min_frame, max_frame + 1)})\n",
    "    \n",
    "    # Merge to ensure that annotated frames retain their original data.\n",
    "    merged = pd.merge(all_frames, df_track, on=\"frame\", how=\"left\")\n",
    "    \n",
    "    # Initialize a column to store interpolated polygon data.\n",
    "    merged[\"interp_polygon\"] = None\n",
    "    \n",
    "    # Identify the indices of rows that already have polygon annotations (i.e., \"points\" are available).\n",
    "    annotated_idx = merged[~merged[\"points\"].isna()].index\n",
    "    \n",
    "    # Process each pair of consecutive annotated frames.\n",
    "    for start_i, end_i in zip(annotated_idx, annotated_idx[1:]):\n",
    "        # Retrieve the frame numbers for the start and end of the current segment.\n",
    "        f_start = merged.loc[start_i, \"frame\"]\n",
    "        f_end = merged.loc[end_i, \"frame\"]\n",
    "        \n",
    "        # Get the polygon points from the start and end frames.\n",
    "        poly_start = merged.loc[start_i, \"points\"]  # Expected to be a list of (x,y) tuples.\n",
    "        poly_end = merged.loc[end_i, \"points\"]\n",
    "        \n",
    "        # Calculate the number of frames between the two annotated frames.\n",
    "        gap = f_end - f_start\n",
    "        \n",
    "        # For each frame in the gap, compute the interpolation factor t and the corresponding polygon.\n",
    "        for offset in range(gap + 1):\n",
    "            # If gap is 0 (shouldn't happen, but for safety), t defaults to 0.\n",
    "            t = offset / float(gap) if gap else 0\n",
    "            # Compute the interpolated polygon using linear interpolation on each vertex.\n",
    "            poly_interp = interpolate_points(poly_start, poly_end, t)\n",
    "            # Store the interpolated polygon in the corresponding row.\n",
    "            merged.at[start_i + offset, \"interp_polygon\"] = poly_interp\n",
    "    \n",
    "    # For frames that were originally annotated, ensure that interp_polygon matches the original points.\n",
    "    merged.loc[annotated_idx, \"interp_polygon\"] = merged.loc[annotated_idx, \"points\"]\n",
    "    \n",
    "    # Propagate constant identification columns from the original annotations to every row.\n",
    "    for col in [\"video_id\", \"polygon_id\", \"name_cvat\", \"polygon_label\", \"type\"]:\n",
    "        if col in df_track.columns:\n",
    "            merged[col] = df_track[col].iloc[0]\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def interpolate_all_polygons(df, num_points=None):\n",
    "    \"\"\"\n",
    "    Interpolate polygon annotations for all polygon tracks in a DataFrame.\n",
    "    \n",
    "    This function processes a DataFrame that contains polygon annotations with columns such as\n",
    "    video_id, polygon_id, frame, and points. It groups the DataFrame by video_id and polygon_track_id,\n",
    "    and then applies interpolation for each group. The result is a DataFrame that contains an interpolated\n",
    "    polygon (\"interp_polygon\") for every frame in the range of each polygon track.\n",
    "    \n",
    "    Note:\n",
    "      - It assumes that the polygon points in \"points\" have the same number of vertices across annotations.\n",
    "        If they do not, a resampling step might be necessary prior to interpolation.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing polygon annotations.\n",
    "        num_points (optional): Parameter reserved for potential resampling of polygon vertices (not used here).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with interpolated polygon annotations for each frame.\n",
    "    \"\"\"\n",
    "    df_result = (\n",
    "        df.groupby([\"video_id\", \"polygon_track_id\"], group_keys=True)\n",
    "          .apply(interpolate_polygon_track)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polygons count before interpolation -> 2658\n",
      "Polygons count before interpolation -> 21237\n"
     ]
    }
   ],
   "source": [
    "print(f'Polygons count before interpolation -> {df_polygons.shape[0]}')\n",
    "df_polygons = interpolate_all_polygons(df_polygons)  # filter for a given video)\n",
    "print(f'Polygons count before interpolation -> {df_polygons.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh731nk_apvv-lung-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
